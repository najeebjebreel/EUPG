{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from datasets import *\n",
    "from mdav import *\n",
    "from train import *\n",
    "from models import *\n",
    "from attacks import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import scipy\n",
    "import csv\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "\n",
    "# Filter out ConvergenceWarning and FitFailedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=7):\n",
    "    np.random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba412321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "columns = [\"age\", \"workClass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "           \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \n",
    "           \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
    "\n",
    "train_data = pd.read_csv('data/adult/adult.data', names=columns, sep=r' *, *', engine='python', na_values='?')\n",
    "test_data = pd.read_csv('data/adult/adult.test', names=columns, sep=r' *, *', skiprows=1, engine='python', na_values='?')\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    (\"num_attr_selector\", ColumnsSelector(type='int')),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"cat_attr_selector\", ColumnsSelector(type='object')),\n",
    "    (\"cat_imputer\", CategoricalImputer(columns=['workClass','occupation', 'native-country'])),\n",
    "    (\"encoder\", CategoricalEncoder(train_data, test_data, dropFirst=True))\n",
    "])\n",
    "\n",
    "full_pipeline = FeatureUnion([(\"num_pipe\", num_pipeline), (\"cat_pipeline\", cat_pipeline)])\n",
    "\n",
    "# Drop useless columns\n",
    "train_data.drop(['fnlwgt', 'education'], axis=1, inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.drop(['fnlwgt', 'education'], axis=1, inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "# copy the data before preprocessing\n",
    "train_copy = train_data.copy()\n",
    "# convert the income column to 0 or 1 and then drop the column for the feature vectors\n",
    "train_copy[\"income\"] = train_copy[\"income\"].apply(lambda x:0 if x=='<=50K' else 1)\n",
    "# creating the feature vector \n",
    "X_train = train_copy.drop('income', axis =1)\n",
    "# target values\n",
    "y_train = train_copy['income'].values\n",
    "# pass the data through the full_pipeline\n",
    "X_train = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "# take a copy of the test data set\n",
    "test_copy = test_data.copy()\n",
    "# convert the income column to 0 or 1\n",
    "test_copy[\"income\"] = test_copy[\"income\"].apply(lambda x:0 if x=='<=50K.' else 1)\n",
    "# separating the feature vecotrs and the target values\n",
    "X_test = test_copy.drop('income', axis =1)\n",
    "y_test = test_copy['income'].values\n",
    "# preprocess the test data using the full pipeline\n",
    "# here we set the type_df param to 'test'\n",
    "X_test = full_pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "# Randomly sample retain and forget sets\n",
    "forget_ratio = 0.05\n",
    "idxs = np.arange(len(y_train))\n",
    "random.shuffle(idxs)\n",
    "m = int(len(y_train)*forget_ratio)\n",
    "retain_idxs = idxs[m:]\n",
    "forget_idxs = idxs[:m]\n",
    "X_retain = X_train[retain_idxs]\n",
    "y_retain = y_train[retain_idxs]\n",
    "X_forget = X_train[forget_idxs]\n",
    "y_forget = y_train[forget_idxs]\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.int64))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.int64))\n",
    "retain_dataset = TensorDataset(torch.tensor(X_retain, dtype=torch.float32), torch.tensor(y_retain, dtype=torch.int64))\n",
    "forget_dataset = TensorDataset(torch.tensor(X_forget, dtype=torch.float32), torch.tensor(y_forget, dtype=torch.int64))\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True)\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y_train) * 100\n",
    "    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))\n",
    "    \n",
    "num_features = X_train.shape[-1]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_model = MLPModel(num_features, 128, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-2\n",
    "n_repeat = 3\n",
    "max_epochs = 100\n",
    "patience = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define and train M on D\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "for r in range(n_repeat):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = copy.deepcopy(initial_model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    t0 = time.time()\n",
    "    model = train_model(model, train_loader, test_loader, criterion, optimizer, \n",
    "                        max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                        patience = patience)\n",
    "\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "    \n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    model.eval()\n",
    "    # Accuracy\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, test_loader)\n",
    "    train_accs.append(100.0*train_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    #MIA\n",
    "    idxs = np.arange(len(test_dataset))\n",
    "    random.shuffle(idxs)\n",
    "    rand_idxs = idxs[:m]\n",
    "    logits_test, loss_test, test_labels = compute_attack_components(model, test_loader)\n",
    "    logits_forget, loss_forget, forget_labels = compute_attack_components(model, forget_loader)\n",
    "    attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                          forget_labels, test_labels[rand_idxs])\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "\n",
    "mean_runtime = np.mean(runtimes)\n",
    "std_runtime = np.std(runtimes)\n",
    "mean_train_acc = np.mean(train_accs)\n",
    "std_train_acc = np.std(train_accs)\n",
    "mean_test_acc = np.mean(test_accs)\n",
    "std_test_acc = np.std(test_accs)\n",
    "mean_mia_auc = np.mean(mia_aucs)\n",
    "std_mia_auc = np.std(mia_aucs)\n",
    "mean_mia_adv = np.mean(mia_advs)\n",
    "std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = 'results/adult/mlp_m_d_fr={}.csv'.format(forget_ratio)\n",
    "\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "    writer.writerow(['Train accuracy', mean_train_acc, std_train_acc])\n",
    "    writer.writerow(['Test accuracy', mean_test_acc, std_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train M_retain on D_retain\n",
    "retain_accs = []\n",
    "forget_accs = []\n",
    "test_accs = []\n",
    "mia_aucs = []\n",
    "mia_advs = []\n",
    "runtimes = []\n",
    "for r in range(n_repeat):\n",
    "    torch.cuda.empty_cache()\n",
    "    model_ret = copy.deepcopy(initial_model)\n",
    "    optimizer = optim.Adam(model_ret.parameters(), lr=lr)\n",
    "    t0 = time.time()\n",
    "    model_ret = train_model(model_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                    max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                        patience = patience)\n",
    "\n",
    "    t1 = time.time()\n",
    "    rt = t1-t0\n",
    "    runtimes.append(rt)\n",
    "    \n",
    "    # Evaluate the model accuracy, and MIA\n",
    "    model_ret.eval()\n",
    "    # Accuracy\n",
    "    retain_acc = accuracy(model_ret, retain_loader)\n",
    "    test_acc = accuracy(model_ret, test_loader)\n",
    "    forget_acc = accuracy(model_ret, forget_loader)\n",
    "    retain_accs.append(100.0*retain_acc)\n",
    "    forget_accs.append(100.0*forget_acc)\n",
    "    test_accs.append(100.0*test_acc)\n",
    "    #MIA\n",
    "    logits_test, loss_test, test_labels = compute_attack_components(model_ret, test_loader)\n",
    "    logits_forget, loss_forget, forget_labels = compute_attack_components(model_ret, forget_loader)\n",
    "    attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                          forget_labels, test_labels[rand_idxs])\n",
    "    auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "    adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "    mia_aucs.append(100.0*auc)\n",
    "    mia_advs.append(100.0*adv)\n",
    "    \n",
    "\n",
    "mean_retrain_runtime = np.mean(runtimes)\n",
    "std_retrain_runtime = np.std(runtimes)\n",
    "mean_retain_acc = np.mean(retain_accs)\n",
    "std_retain_acc = np.std(retain_accs)\n",
    "mean_forget_acc = np.mean(forget_accs)\n",
    "std_forget_acc = np.std(forget_accs)\n",
    "mean_retrain_test_acc = np.mean(test_accs)\n",
    "std_retrain_test_acc = np.std(test_accs)\n",
    "mean_retrain_mia_auc = np.mean(mia_aucs)\n",
    "std_retrain_mia_auc = np.std(mia_aucs)\n",
    "mean_retrain_mia_adv = np.mean(mia_advs)\n",
    "std_retrain_mia_adv = np.std(mia_advs)\n",
    "\n",
    "# Print the results\n",
    "print('Retraining M on D_ret time:{:0.2f}(±{:0.2f}) seconds'.format(mean_retrain_runtime, std_retrain_runtime))\n",
    "print('Retain accuracy:{:0.2f}(±{:0.2f})%'.format(mean_retain_acc, std_retain_acc))\n",
    "print('Forget accuracy:{:0.2f}(±{:0.2f})%'.format(mean_forget_acc, std_forget_acc))\n",
    "print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_retrain_test_acc, std_retrain_test_acc))\n",
    "print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_retrain_mia_auc, std_retrain_mia_auc))\n",
    "print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_retrain_mia_adv, std_retrain_mia_adv))\n",
    "\n",
    "# Save to CSV\n",
    "csv_retrain_file_path = 'results/adult/mlp_mret_dret_fr={}.csv'.format(forget_ratio)\n",
    "\n",
    "with open(csv_retrain_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "    writer.writerow(['Retraining Time', mean_retrain_runtime, std_retrain_runtime])\n",
    "    writer.writerow(['Retain accuracy', mean_retain_acc, std_retain_acc])\n",
    "    writer.writerow(['Forget accuracy', mean_forget_acc, std_forget_acc])\n",
    "    writer.writerow(['Test accuracy', mean_retrain_test_acc, std_retrain_test_acc])\n",
    "    writer.writerow(['MIA AUC', mean_retrain_mia_auc, std_retrain_mia_auc])\n",
    "    writer.writerow(['MIA Advantage', mean_retrain_mia_adv, std_retrain_mia_adv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9287fc3",
   "metadata": {},
   "source": [
    "# k-anonymity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7487244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: k-anonymize and prepare D_k\n",
    "ft_epochs_list = [5, 10, 20]\n",
    "for ft_epochs in ft_epochs_list:\n",
    "    K = [3, 5, 10, 20, 80]\n",
    "    for k in K:\n",
    "        runtimes_k = []\n",
    "        t0 = time.time()\n",
    "        centroids, clusters, labels, X_train_k, y_train_k = mdav(copy.deepcopy(X_train), copy.deepcopy(y_train), k)\n",
    "        print('Shape of X_train_k:{}, y_train_k:{}'.format(X_train_k.shape, y_train_k.shape))\n",
    "         # Create TensorDatasets\n",
    "        train_dataset_k = TensorDataset(torch.tensor(X_train_k, dtype=torch.float32), torch.tensor(y_train_k, dtype=torch.int64))\n",
    "        train_loader_k = DataLoader(train_dataset_k, batch_size=batch_size, shuffle=True)\n",
    "        t1 = time.time()\n",
    "        rt_k = t1- t0\n",
    "        runtimes_k.append(rt_k)\n",
    "\n",
    "        train_accs_k = []\n",
    "        test_accs_k = []\n",
    "        mia_aucs_k = []\n",
    "        mia_advs_k = []\n",
    "        runtimes_train_k = []\n",
    "\n",
    "        train_accs_k_D = []\n",
    "        test_accs_k_D = []\n",
    "        mia_aucs_k_D = []\n",
    "        mia_advs_k_D = []\n",
    "        runtimes_train_k_D = []\n",
    "\n",
    "        retain_accs_k_ret = []\n",
    "        forget_accs_k_ret = []\n",
    "        test_accs_k_ret = []\n",
    "        mia_aucs_k_ret = []\n",
    "        mia_advs_k_ret = []\n",
    "        runtimes_train_k_ret = []\n",
    "\n",
    "        for r in range(n_repeat):\n",
    "            torch.cuda.empty_cache()\n",
    "            model_k = copy.deepcopy(initial_model)\n",
    "            optimizer = optim.Adam(model_k.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k = train_model(model_k, train_loader_k, test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt_train = t1- t0\n",
    "            runtimes_train_k.append(rt_train)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k.eval()\n",
    "            #Accuracy\n",
    "            train_acc = accuracy(model_k, train_loader)\n",
    "            test_acc = accuracy(model_k, test_loader)\n",
    "            train_accs_k.append(100.0*train_acc)\n",
    "            test_accs_k.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k.append(100.0*auc)\n",
    "            mia_advs_k.append(100.0*adv)\n",
    "\n",
    "            model_k_D = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_D.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_D = train_model(model_k_D, train_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_D.append(rt)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_D.eval()\n",
    "            #Accuracy\n",
    "            train_acc = accuracy(model_k_D, train_loader)\n",
    "            test_acc = accuracy(model_k_D, test_loader)\n",
    "            train_accs_k_D.append(100.0*train_acc)\n",
    "            test_accs_k_D.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_D, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_D, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_D.append(100.0*auc)\n",
    "            mia_advs_k_D.append(100.0*adv)\n",
    "\n",
    "            model_k_ret = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_ret.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_ret = train_model(model_k_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_ret.append(rt)\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_ret.eval()\n",
    "            #Accuracy\n",
    "            retain_acc = accuracy(model_k_ret, retain_loader)\n",
    "            forget_acc = accuracy(model_k_ret, forget_loader)\n",
    "            test_acc = accuracy(model_k_ret, test_loader)\n",
    "            retain_accs_k_ret.append(100.0*retain_acc)\n",
    "            forget_accs_k_ret.append(100.0*forget_acc)\n",
    "            test_accs_k_ret.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_ret, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_ret, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_ret.append(100.0*auc)\n",
    "            mia_advs_k_ret.append(100.0*adv)\n",
    "\n",
    "\n",
    "        # Anonymizing D and training M_k on D_k\n",
    "        mean_anonymize_time = np.mean(runtimes_k)\n",
    "        std_anonymize_time = np.std(runtimes_k)\n",
    "        mean_train_k_time = np.mean(runtimes_train_k)\n",
    "        std_train_k_time = np.std(runtimes_train_k)\n",
    "        mean_train_k_acc = np.mean(train_accs_k)\n",
    "        std_train_k_acc = np.std(train_accs_k)\n",
    "        mean_test_k_acc = np.mean(test_accs_k)\n",
    "        std_test_k_acc = np.std(test_accs_k)\n",
    "        mean_mia_k_auc = np.mean(mia_aucs_k)\n",
    "        std_mia_k_auc = np.std(mia_aucs_k)\n",
    "        mean_mia_k_adv = np.mean(mia_advs_k)\n",
    "        std_mia_k_adv = np.std(mia_advs_k)\n",
    "\n",
    "        # Finetuning M_k on D\n",
    "        mean_finetune_D_time = np.mean(runtimes_train_k_D)\n",
    "        std_finetune_D_time = np.std(runtimes_train_k_D)\n",
    "        mean_finetune_D_train_acc = np.mean(train_accs_k_D)\n",
    "        std_finetune_D_train_acc = np.std(train_accs_k_D)\n",
    "        mean_finetune_D_test_acc = np.mean(test_accs_k_D)\n",
    "        std_finetune_D_test_acc = np.std(test_accs_k_D)\n",
    "        mean_finetune_D_mia_auc = np.mean(mia_aucs_k_D)\n",
    "        std_finetune_D_mia_auc = np.std(mia_aucs_k_D)\n",
    "        mean_finetune_D_mia_adv = np.mean(mia_advs_k_D)\n",
    "        std_finetune_D_mia_adv = np.std(mia_advs_k_D)\n",
    "\n",
    "        # Finetuning M_k on D_ret\n",
    "        mean_finetune_D_ret_time = np.mean(runtimes_train_k_ret)\n",
    "        std_finetune_D_ret_time = np.std(runtimes_train_k_ret)\n",
    "        mean_finetune_D_ret_train_acc = np.mean(retain_accs_k_ret)\n",
    "        std_finetune_D_ret_train_acc = np.std(retain_accs_k_ret)\n",
    "        mean_finetune_D_ret_forget_acc = np.mean(forget_accs_k_ret)\n",
    "        std_finetune_D_ret_forget_acc = np.std(forget_accs_k_ret)\n",
    "        mean_finetune_D_ret_test_acc = np.mean(test_accs_k_ret)\n",
    "        std_finetune_D_ret_test_acc = np.std(test_accs_k_ret)\n",
    "        mean_finetune_D_ret_mia_auc = np.mean(mia_aucs_k_ret)\n",
    "        std_finetune_D_ret_mia_auc = np.std(mia_aucs_k_ret)\n",
    "        mean_finetune_D_ret_mia_adv = np.mean(mia_advs_k_ret)\n",
    "        std_finetune_D_ret_mia_adv = np.std(mia_advs_k_ret)\n",
    "\n",
    "\n",
    "\n",
    "        # Print the results\n",
    "        print('----------------------------------------')\n",
    "        print('k=', k, 'Fine-tuning epochs=', ft_epochs)\n",
    "        print('----------------------------------------')\n",
    "        print('-----Anonymizing D and training M_k on D_k-----')\n",
    "        print('Anonymizing D time:{:0.2f}(±{:0.2f})'.format(mean_anonymize_time, std_anonymize_time))\n",
    "        print('Training M_k on D_k time:{:0.2f}(±{:0.2f})'.format(mean_train_k_time, std_train_k_time))\n",
    "        print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_k_acc, std_train_k_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_k_acc, std_test_k_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_auc, std_mia_k_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_adv, std_mia_k_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D-----')\n",
    "        print('Training M_k on D time:{:0.2f}(±{:0.2f})'.format(mean_finetune_D_time, std_finetune_D_time))\n",
    "        print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_train_acc, std_finetune_D_train_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_test_acc, std_finetune_D_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_auc, std_finetune_D_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_adv, std_finetune_D_mia_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D_ret-----')\n",
    "        print('Finetuning M_k on D_retain time:{:0.2f}(±{:0.2f}) seconds'.format(mean_finetune_D_ret_time, std_finetune_D_ret_time))\n",
    "        print('Retain accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc))\n",
    "        print('Forget accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_anonymize_file_path = 'results/adult/mlp_mk={}_dk_fr={}.csv'.format(k, forget_ratio)\n",
    "        csv_finetune_D_file_path = 'results/adult/mlp_mk={}_d_fr={}_epochs={}.csv'.format(k, forget_ratio, ft_epochs)\n",
    "        csv_finetune_D_ret_file_path = 'results/adult/mlp_mk={}_dret_fr={}_epochs={}.csv'.format(k, forget_ratio, ft_epochs)\n",
    "\n",
    "        # Writing to CSV for anonymizing, finetuning on D, and finetuning on D_ret\n",
    "        with open(csv_anonymize_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Anonymizing Time', mean_anonymize_time, std_anonymize_time])\n",
    "            writer.writerow(['Training M_k on D_k Time', mean_train_k_time, std_train_k_time])\n",
    "            writer.writerow(['Train accuracy', mean_train_k_acc, std_train_k_acc])\n",
    "            writer.writerow(['Test accuracy', mean_test_k_acc, std_test_k_acc])\n",
    "            writer.writerow(['MIA AUC', mean_mia_k_auc, std_mia_k_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_mia_k_adv, std_mia_k_adv])\n",
    "\n",
    "        with open(csv_finetune_D_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Training M_k on D Time', mean_finetune_D_time, std_finetune_D_time])\n",
    "            writer.writerow(['Train accuracy', mean_finetune_D_train_acc, std_finetune_D_train_acc])\n",
    "            writer.writerow(['Test accuracy', mean_finetune_D_test_acc, std_finetune_D_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_mia_auc, std_finetune_D_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_mia_adv, std_finetune_D_mia_adv])\n",
    "\n",
    "        with open(csv_finetune_D_ret_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Finetuning M_k on D_retain Time', mean_finetune_D_ret_time, std_finetune_D_ret_time])\n",
    "            writer.writerow(['Retain accuracy', mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc])\n",
    "            writer.writerow(['Forget accuracy', mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc])\n",
    "            writer.writerow(['Test accuracy', mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c035b",
   "metadata": {},
   "source": [
    "# Differential privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: k-anonymize and prepare D_k\n",
    "ft_epochs_list = [5, 10, 20]\n",
    "for ft_epochs in ft_epochs_list:\n",
    "    EPS = [0.5, 2.5, 5.0, 25.0, 50.0, 100.0]\n",
    "    for eps in EPS:\n",
    "        dp_train_data = pd.read_csv('dp_data/adult/dp_adult_eps={}.csv'.format(eps), sep=r' *, *', engine='python', na_values='?')\n",
    "        dp_train_data.head()\n",
    "        # Drop useless columns\n",
    "        dp_train_data.drop(['fnlwgt', 'education'], axis=1, inplace=True)\n",
    "        dp_train_data.dropna(inplace=True)\n",
    "        # convert the income column to 0 or 1 and then drop the column for the feature vectors\n",
    "        dp_train_data[\"income\"] = dp_train_data[\"income\"].apply(lambda x:0 if x=='<=50K' else 1)\n",
    "        # creating the feature vector \n",
    "        X_train_dp = dp_train_data.drop('income', axis =1)\n",
    "        # target values\n",
    "        y_train_dp = dp_train_data['income'].values\n",
    "        # pass the data through the full_pipeline\n",
    "        X_train_dp = full_pipeline.fit_transform(X_train_dp)\n",
    "        # Create TensorDatasets\n",
    "        train_dataset_k = TensorDataset(torch.tensor(X_train_dp, dtype=torch.float32), torch.tensor(y_train_dp, dtype=torch.int64))\n",
    "        train_loader_k = DataLoader(train_dataset_k, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        train_accs_k = []\n",
    "        test_accs_k = []\n",
    "        mia_aucs_k = []\n",
    "        mia_advs_k = []\n",
    "        runtimes_train_k = []\n",
    "\n",
    "        train_accs_k_D = []\n",
    "        test_accs_k_D = []\n",
    "        mia_aucs_k_D = []\n",
    "        mia_advs_k_D = []\n",
    "        runtimes_train_k_D = []\n",
    "\n",
    "        retain_accs_k_ret = []\n",
    "        forget_accs_k_ret = []\n",
    "        test_accs_k_ret = []\n",
    "        mia_aucs_k_ret = []\n",
    "        mia_advs_k_ret = []\n",
    "        runtimes_train_k_ret = []\n",
    "\n",
    "        for r in range(n_repeat):\n",
    "            torch.cuda.empty_cache()\n",
    "            model_k = copy.deepcopy(initial_model)\n",
    "            optimizer = optim.Adam(model_k.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k = train_model(model_k, train_loader_k, test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt_train = t1- t0\n",
    "            runtimes_train_k.append(rt_train)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k.eval()\n",
    "            #Accuracy\n",
    "            train_acc = accuracy(model_k, train_loader)\n",
    "            test_acc = accuracy(model_k, test_loader)\n",
    "            train_accs_k.append(100.0*train_acc)\n",
    "            test_accs_k.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            idxs = np.arange(len(test_dataset))\n",
    "            random.shuffle(idxs)\n",
    "            rand_idxs = idxs[:m]\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k.append(100.0*auc)\n",
    "            mia_advs_k.append(100.0*adv)\n",
    "\n",
    "            model_k_D = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_D.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_D = train_model(model_k_D, train_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_D.append(rt)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_D.eval()\n",
    "            #Accuracy\n",
    "            train_acc = accuracy(model_k_D, train_loader)\n",
    "            test_acc = accuracy(model_k_D, test_loader)\n",
    "            train_accs_k_D.append(100.0*train_acc)\n",
    "            test_accs_k_D.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_D, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_D, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_D.append(100.0*auc)\n",
    "            mia_advs_k_D.append(100.0*adv)\n",
    "\n",
    "            model_k_ret = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_ret.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_ret = train_model(model_k_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_ret.append(rt)\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_ret.eval()\n",
    "            #Accuracy\n",
    "            retain_acc = accuracy(model_k_ret, retain_loader)\n",
    "            forget_acc = accuracy(model_k_ret, forget_loader)\n",
    "            test_acc = accuracy(model_k_ret, test_loader)\n",
    "            retain_accs_k_ret.append(100.0*retain_acc)\n",
    "            forget_accs_k_ret.append(100.0*forget_acc)\n",
    "            test_accs_k_ret.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_ret, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_ret, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_ret.append(100.0*auc)\n",
    "            mia_advs_k_ret.append(100.0*adv)\n",
    "\n",
    "\n",
    "        # Anonymizing D and training M_k on D_k\n",
    "        mean_train_k_time = np.mean(runtimes_train_k)\n",
    "        std_train_k_time = np.std(runtimes_train_k)\n",
    "        mean_train_k_acc = np.mean(train_accs_k)\n",
    "        std_train_k_acc = np.std(train_accs_k)\n",
    "        mean_test_k_acc = np.mean(test_accs_k)\n",
    "        std_test_k_acc = np.std(test_accs_k)\n",
    "        mean_mia_k_auc = np.mean(mia_aucs_k)\n",
    "        std_mia_k_auc = np.std(mia_aucs_k)\n",
    "        mean_mia_k_adv = np.mean(mia_advs_k)\n",
    "        std_mia_k_adv = np.std(mia_advs_k)\n",
    "\n",
    "        # Finetuning M_k on D\n",
    "        mean_finetune_D_time = np.mean(runtimes_train_k_D)\n",
    "        std_finetune_D_time = np.std(runtimes_train_k_D)\n",
    "        mean_finetune_D_train_acc = np.mean(train_accs_k_D)\n",
    "        std_finetune_D_train_acc = np.std(train_accs_k_D)\n",
    "        mean_finetune_D_test_acc = np.mean(test_accs_k_D)\n",
    "        std_finetune_D_test_acc = np.std(test_accs_k_D)\n",
    "        mean_finetune_D_mia_auc = np.mean(mia_aucs_k_D)\n",
    "        std_finetune_D_mia_auc = np.std(mia_aucs_k_D)\n",
    "        mean_finetune_D_mia_adv = np.mean(mia_advs_k_D)\n",
    "        std_finetune_D_mia_adv = np.std(mia_advs_k_D)\n",
    "\n",
    "        # Finetuning M_k on D_ret\n",
    "        mean_finetune_D_ret_time = np.mean(runtimes_train_k_ret)\n",
    "        std_finetune_D_ret_time = np.std(runtimes_train_k_ret)\n",
    "        mean_finetune_D_ret_train_acc = np.mean(retain_accs_k_ret)\n",
    "        std_finetune_D_ret_train_acc = np.std(retain_accs_k_ret)\n",
    "        mean_finetune_D_ret_forget_acc = np.mean(forget_accs_k_ret)\n",
    "        std_finetune_D_ret_forget_acc = np.std(forget_accs_k_ret)\n",
    "        mean_finetune_D_ret_test_acc = np.mean(test_accs_k_ret)\n",
    "        std_finetune_D_ret_test_acc = np.std(test_accs_k_ret)\n",
    "        mean_finetune_D_ret_mia_auc = np.mean(mia_aucs_k_ret)\n",
    "        std_finetune_D_ret_mia_auc = np.std(mia_aucs_k_ret)\n",
    "        mean_finetune_D_ret_mia_adv = np.mean(mia_advs_k_ret)\n",
    "        std_finetune_D_ret_mia_adv = np.std(mia_advs_k_ret)\n",
    "\n",
    "        # Print the results\n",
    "        print('----------------------------------------')\n",
    "        print('Epsilon=', eps, 'Fine-tuning epochs=', ft_epochs)\n",
    "        print('----------------------------------------')\n",
    "        print('-----Anonymizing D and training M_dp on D_dp-----')\n",
    "        print('Training M_k on D_k time:{:0.2f}(±{:0.2f})'.format(mean_train_k_time, std_train_k_time))\n",
    "        print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_train_k_acc, std_train_k_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_test_k_acc, std_test_k_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_auc, std_mia_k_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_adv, std_mia_k_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D-----')\n",
    "        print('Training M_k on D time:{:0.2f}(±{:0.2f})'.format(mean_finetune_D_time, std_finetune_D_time))\n",
    "        print('Train accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_train_acc, std_finetune_D_train_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_test_acc, std_finetune_D_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_auc, std_finetune_D_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_adv, std_finetune_D_mia_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D_ret-----')\n",
    "        print('Finetuning M_k on D_retain time:{:0.2f}(±{:0.2f}) seconds'.format(mean_finetune_D_ret_time, std_finetune_D_ret_time))\n",
    "        print('Retain accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc))\n",
    "        print('Forget accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc))\n",
    "        print('Test accuracy:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_anonymize_file_path = 'results/adult/mlp_mdp_eps={}_fr={}.csv'.format(eps, forget_ratio)\n",
    "        csv_finetune_D_file_path = 'results/adult/mlp_mdpd_eps={}_fr={}_epochs={}.csv'.format(eps, forget_ratio, ft_epochs)\n",
    "        csv_finetune_D_ret_file_path = 'results/adult/mlp_mdpret_eps={}_fr={}_epochs={}.csv'.format(eps, forget_ratio, ft_epochs)\n",
    "\n",
    "        # Writing to CSV for anonymizing, finetuning on D, and finetuning on D_ret\n",
    "        with open(csv_anonymize_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Training M_k on D_k Time', mean_train_k_time, std_train_k_time])\n",
    "            writer.writerow(['Train Accuracy', mean_train_k_acc, std_train_k_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_test_k_acc, std_test_k_acc])\n",
    "            writer.writerow(['MIA AUC', mean_mia_k_auc, std_mia_k_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_mia_k_adv, std_mia_k_adv])\n",
    "\n",
    "        with open(csv_finetune_D_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Training M_k on D Time', mean_finetune_D_time, std_finetune_D_time])\n",
    "            writer.writerow(['Train Accuracy', mean_finetune_D_train_acc, std_finetune_D_train_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_finetune_D_test_acc, std_finetune_D_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_mia_auc, std_finetune_D_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_mia_adv, std_finetune_D_mia_adv])\n",
    "\n",
    "        with open(csv_finetune_D_ret_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Finetuning M_k on D_retain Time', mean_finetune_D_ret_time, std_finetune_D_ret_time])\n",
    "            writer.writerow(['Retain Accuracy', mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc])\n",
    "            writer.writerow(['Forget Accuracy', mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc])\n",
    "            writer.writerow(['Test Accuracy', mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c0876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lira-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
