{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "from utils import *\n",
    "from datasets import *\n",
    "from mdav import *\n",
    "from train import *\n",
    "from models import *\n",
    "from attacks import *\n",
    "\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1531e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "\n",
    "# Filter out ConvergenceWarning and FitFailedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6937e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=7):\n",
    "    np.random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba412321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get dataset\n",
    "\n",
    "df_train=pd.read_csv('data/GiveMeSomeCredit/cs-training.csv')\n",
    "df_train.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_train.dropna(inplace=True)\n",
    "y = df_train['SeriousDlqin2yrs'].values\n",
    "df_train.drop(df_train[['SeriousDlqin2yrs']],axis=1,inplace=True)\n",
    "X = df_train.values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "print('X_train shape', X_train.shape)\n",
    "print('X_train unique shape', np.unique(X_train, axis = 0).shape)\n",
    "print('X_test shape', X_test.shape)\n",
    "print('X_test unique shape', np.unique(X_test, axis = 0).shape)\n",
    "\n",
    "SC = StandardScaler()\n",
    "X_train = SC.fit_transform(X_train)\n",
    "X_test = SC.transform(X_test)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.int64))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.int64))\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_features = X_train.shape[-1]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y_train) * 100\n",
    "    print('Class=%s, Count=%d, Percentage=%.2f%%' % (k, v, per))\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_model = MLPModel(num_features, 256, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "n_repeat = 1\n",
    "max_epochs = 200\n",
    "patience = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample retain and forget sets\n",
    "forget_ratios = [0.8]\n",
    "for forget_ratio in forget_ratios:\n",
    "    m = int(len(y_train)*forget_ratio)\n",
    "    idxs = np.arange(len(y_train))\n",
    "    random.shuffle(idxs)\n",
    "\n",
    "    retain_idxs = idxs[m:]\n",
    "    forget_idxs = idxs[:m]\n",
    "    X_retain = X_train[retain_idxs]\n",
    "    y_retain = y_train[retain_idxs]\n",
    "    X_forget = X_train[forget_idxs]\n",
    "    y_forget = y_train[forget_idxs]\n",
    "    print('X_retain shape', X_retain.shape)\n",
    "    print('X_retain unique shape', np.unique(X_retain, axis = 0).shape)\n",
    "    print('X_forget shape', X_forget.shape)\n",
    "    print('X_forget unique shape', np.unique(X_forget, axis = 0).shape)\n",
    "    retain_dataset = TensorDataset(torch.tensor(X_retain, dtype=torch.float32), torch.tensor(y_retain, dtype=torch.int64))\n",
    "    forget_dataset = TensorDataset(torch.tensor(X_forget, dtype=torch.float32), torch.tensor(y_forget, dtype=torch.int64))\n",
    "\n",
    "    # Create DataLoader instances\n",
    "    retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True)\n",
    "    forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Step 2: Define and train M on D\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    mia_aucs = []\n",
    "    mia_advs = []\n",
    "    runtimes = []\n",
    "    for r in range(n_repeat):\n",
    "        torch.cuda.empty_cache()\n",
    "        model = copy.deepcopy(initial_model)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        t0 = time.time()\n",
    "        model = train_model(model, train_loader, test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "\n",
    "        t1 = time.time()\n",
    "        rt = t1-t0\n",
    "        runtimes.append(rt)\n",
    "\n",
    "        # Evaluate the model accuracy, and MIA\n",
    "        model.eval()\n",
    "        # Accuracy\n",
    "        train_acc = auc_score(model, train_loader)\n",
    "        test_acc = auc_score(model, test_loader)\n",
    "        train_accs.append(100.0*train_acc)\n",
    "        test_accs.append(100.0*test_acc)\n",
    "        #MIA\n",
    "        idxs = np.arange(len(test_dataset))\n",
    "        random.shuffle(idxs)\n",
    "        rand_idxs = idxs[:m]\n",
    "        logits_test, loss_test, test_labels = compute_attack_components(model, test_loader)\n",
    "        logits_forget, loss_forget, forget_labels = compute_attack_components(model, forget_loader)\n",
    "        attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                              forget_labels, test_labels[rand_idxs])\n",
    "        auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "        adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "        mia_aucs.append(100.0*auc)\n",
    "        mia_advs.append(100.0*adv)\n",
    "\n",
    "    mean_runtime = np.mean(runtimes)\n",
    "    std_runtime = np.std(runtimes)\n",
    "    mean_train_acc = np.mean(train_accs)\n",
    "    std_train_acc = np.std(train_accs)\n",
    "    mean_test_acc = np.mean(test_accs)\n",
    "    std_test_acc = np.std(test_accs)\n",
    "    mean_mia_auc = np.mean(mia_aucs)\n",
    "    std_mia_auc = np.std(mia_aucs)\n",
    "    mean_mia_adv = np.mean(mia_advs)\n",
    "    std_mia_adv = np.std(mia_advs)\n",
    "\n",
    "    # Print the results\n",
    "    print('Training M on D time:{:0.2f}(±{:0.2f}) seconds'.format(mean_runtime, std_runtime))\n",
    "    print('Train AUC:{:0.2f}(±{:0.2f})%'.format(mean_train_acc, std_train_acc))\n",
    "    print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_test_acc, std_test_acc))\n",
    "    print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_auc, std_mia_auc))\n",
    "    print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_adv, std_mia_adv))\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_file_path = 'results/credit/mlp_m_d_fr={}.csv'.format(forget_ratio)\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "        writer.writerow(['Training Time', mean_runtime, std_runtime])\n",
    "        writer.writerow(['Train AUC', mean_train_acc, std_train_acc])\n",
    "        writer.writerow(['Test AUC', mean_test_acc, std_test_acc])\n",
    "        writer.writerow(['MIA AUC', mean_mia_auc, std_mia_auc])\n",
    "        writer.writerow(['MIA Advantage', mean_mia_adv, std_mia_adv])\n",
    "######################################################################################################\n",
    "    # Step 3: Train M_retain on D_retain\n",
    "    retain_accs = []\n",
    "    forget_accs = []\n",
    "    test_accs = []\n",
    "    mia_aucs = []\n",
    "    mia_advs = []\n",
    "    runtimes = []\n",
    "    for r in range(n_repeat):\n",
    "        torch.cuda.empty_cache()\n",
    "        model_ret = copy.deepcopy(initial_model)\n",
    "        optimizer = optim.Adam(model_ret.parameters(), lr=lr)\n",
    "        t0 = time.time()\n",
    "        model_ret = train_model(model_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                        max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "\n",
    "        t1 = time.time()\n",
    "        rt = t1-t0\n",
    "        runtimes.append(rt)\n",
    "\n",
    "        # Evaluate the model accuracy, and MIA\n",
    "        model_ret.eval()\n",
    "        # Accuracy\n",
    "        retain_acc = auc_score(model_ret, retain_loader)\n",
    "        test_acc = auc_score(model_ret, test_loader)\n",
    "        forget_acc = auc_score(model_ret, forget_loader)\n",
    "        retain_accs.append(100.0*retain_acc)\n",
    "        forget_accs.append(100.0*forget_acc)\n",
    "        test_accs.append(100.0*test_acc)\n",
    "        #MIA\n",
    "        logits_test, loss_test, test_labels = compute_attack_components(model_ret, test_loader)\n",
    "        logits_forget, loss_forget, forget_labels = compute_attack_components(model_ret, forget_loader)\n",
    "        attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                              forget_labels, test_labels[rand_idxs])\n",
    "        auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "        adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "        mia_aucs.append(100.0*auc)\n",
    "        mia_advs.append(100.0*adv)\n",
    "\n",
    "\n",
    "    mean_retrain_runtime = np.mean(runtimes)\n",
    "    std_retrain_runtime = np.std(runtimes)\n",
    "    mean_retain_acc = np.mean(retain_accs)\n",
    "    std_retain_acc = np.std(retain_accs)\n",
    "    mean_forget_acc = np.mean(forget_accs)\n",
    "    std_forget_acc = np.std(forget_accs)\n",
    "    mean_retrain_test_acc = np.mean(test_accs)\n",
    "    std_retrain_test_acc = np.std(test_accs)\n",
    "    mean_retrain_mia_auc = np.mean(mia_aucs)\n",
    "    std_retrain_mia_auc = np.std(mia_aucs)\n",
    "    mean_retrain_mia_adv = np.mean(mia_advs)\n",
    "    std_retrain_mia_adv = np.std(mia_advs)\n",
    "\n",
    "    # Print the results\n",
    "    print('Retraining M on D_ret time:{:0.2f}(±{:0.2f}) seconds'.format(mean_retrain_runtime, std_retrain_runtime))\n",
    "    print('Retain AUC:{:0.2f}(±{:0.2f})%'.format(mean_retain_acc, std_retain_acc))\n",
    "    print('Forget AUC:{:0.2f}(±{:0.2f})%'.format(mean_forget_acc, std_forget_acc))\n",
    "    print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_retrain_test_acc, std_retrain_test_acc))\n",
    "    print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_retrain_mia_auc, std_retrain_mia_auc))\n",
    "    print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_retrain_mia_adv, std_retrain_mia_adv))\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_retrain_file_path = 'results/credit/mlp_mret_dret_fr={}.csv'.format(forget_ratio)\n",
    "\n",
    "    with open(csv_retrain_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "        writer.writerow(['Retraining Time', mean_retrain_runtime, std_retrain_runtime])\n",
    "        writer.writerow(['Retain AUC', mean_retain_acc, std_retain_acc])\n",
    "        writer.writerow(['Forget AUC', mean_forget_acc, std_forget_acc])\n",
    "        writer.writerow(['Test AUC', mean_retrain_test_acc, std_retrain_test_acc])\n",
    "        writer.writerow(['MIA AUC', mean_retrain_mia_auc, std_retrain_mia_auc])\n",
    "        writer.writerow(['MIA Advantage', mean_retrain_mia_adv, std_retrain_mia_adv])\n",
    "####################################################################################################\n",
    "    # Step 1: k-anonymize and prepare D_k\n",
    "    ft_epochs_list = [5]\n",
    "    for ft_epochs in ft_epochs_list:\n",
    "        K = [10]\n",
    "        for k in K:\n",
    "            runtimes_k = []\n",
    "            t0 = time.time()\n",
    "            centroids, clusters, labels, X_train_k, y_train_k = mdav(copy.deepcopy(X_train), copy.deepcopy(y_train), k)\n",
    "            print_stats(clusters, centroids)\n",
    "            print('Shape of X_train_k:{}, y_train_k:{}'.format(X_train_k.shape, y_train_k.shape))\n",
    "             # Create TensorDatasets\n",
    "            train_dataset_k = TensorDataset(torch.tensor(X_train_k, dtype=torch.float32), torch.tensor(y_train_k, dtype=torch.int64))\n",
    "            train_loader_k = DataLoader(train_dataset_k, batch_size=batch_size, shuffle=True)\n",
    "            t1 = time.time()\n",
    "            rt_k = t1- t0\n",
    "            runtimes_k.append(rt_k)\n",
    "\n",
    "            train_accs_k = []\n",
    "            test_accs_k = []\n",
    "            mia_aucs_k = []\n",
    "            mia_advs_k = []\n",
    "            runtimes_train_k = []\n",
    "\n",
    "            train_accs_k_D = []\n",
    "            test_accs_k_D = []\n",
    "            mia_aucs_k_D = []\n",
    "            mia_advs_k_D = []\n",
    "            runtimes_train_k_D = []\n",
    "\n",
    "            retain_accs_k_ret = []\n",
    "            forget_accs_k_ret = []\n",
    "            test_accs_k_ret = []\n",
    "            mia_aucs_k_ret = []\n",
    "            mia_advs_k_ret = []\n",
    "            runtimes_train_k_ret = []\n",
    "\n",
    "            for r in range(n_repeat):\n",
    "                torch.cuda.empty_cache()\n",
    "                model_k = copy.deepcopy(initial_model)\n",
    "                optimizer = optim.Adam(model_k.parameters(), lr=lr)\n",
    "                t0 = time.time()\n",
    "                model_k = train_model(model_k, train_loader_k, test_loader, criterion, optimizer, \n",
    "                                max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                patience = patience)\n",
    "\n",
    "                t1 = time.time()\n",
    "                rt_train = t1- t0\n",
    "                runtimes_train_k.append(rt_train)\n",
    "\n",
    "                # Evaluate the model accuracy, and MIA\n",
    "                model_k.eval()\n",
    "                #Accuracy\n",
    "                train_acc = auc_score(model_k, train_loader)\n",
    "                test_acc = auc_score(model_k, test_loader)\n",
    "                train_accs_k.append(100.0*train_acc)\n",
    "                test_accs_k.append(100.0*test_acc)\n",
    "                #MIA\n",
    "                logits_test, loss_test, test_labels = compute_attack_components(model_k, test_loader)\n",
    "                logits_forget, loss_forget, forget_labels = compute_attack_components(model_k, forget_loader)\n",
    "                attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                      forget_labels, test_labels[rand_idxs])\n",
    "                auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                mia_aucs_k.append(100.0*auc)\n",
    "                mia_advs_k.append(100.0*adv)\n",
    "\n",
    "                model_k_D = copy.deepcopy(model_k)\n",
    "                torch.cuda.empty_cache()\n",
    "                optimizer = optim.Adam(model_k_D.parameters(), lr=lr)\n",
    "                t0 = time.time()\n",
    "                model_k_D = train_model(model_k_D, train_loader, test_loader, criterion, optimizer, \n",
    "                                    ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                      patience = patience)\n",
    "\n",
    "                t1 = time.time()\n",
    "                rt = t1-t0\n",
    "                runtimes_train_k_D.append(rt)\n",
    "\n",
    "                # Evaluate the model accuracy, and MIA\n",
    "                model_k_D.eval()\n",
    "                #Accuracy\n",
    "                train_acc = auc_score(model_k_D, train_loader)\n",
    "                test_acc = auc_score(model_k_D, test_loader)\n",
    "                train_accs_k_D.append(100.0*train_acc)\n",
    "                test_accs_k_D.append(100.0*test_acc)\n",
    "                #MIA\n",
    "                logits_test, loss_test, test_labels = compute_attack_components(model_k_D, test_loader)\n",
    "                logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_D, forget_loader)\n",
    "                attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                      forget_labels, test_labels[rand_idxs])\n",
    "                auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                mia_aucs_k_D.append(100.0*auc)\n",
    "                mia_advs_k_D.append(100.0*adv)\n",
    "\n",
    "                model_k_ret = copy.deepcopy(model_k)\n",
    "                torch.cuda.empty_cache()\n",
    "                optimizer = optim.Adam(model_k_ret.parameters(), lr=lr)\n",
    "                t0 = time.time()\n",
    "                model_k_ret = train_model(model_k_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                                    ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                      patience = patience)\n",
    "\n",
    "                t1 = time.time()\n",
    "                rt = t1-t0\n",
    "                runtimes_train_k_ret.append(rt)\n",
    "                # Evaluate the model accuracy, and MIA\n",
    "                model_k_ret.eval()\n",
    "                #Accuracy\n",
    "                retain_acc = auc_score(model_k_ret, retain_loader)\n",
    "                forget_acc = auc_score(model_k_ret, forget_loader)\n",
    "                test_acc = auc_score(model_k_ret, test_loader)\n",
    "                retain_accs_k_ret.append(100.0*retain_acc)\n",
    "                forget_accs_k_ret.append(100.0*forget_acc)\n",
    "                test_accs_k_ret.append(100.0*test_acc)\n",
    "                #MIA\n",
    "                logits_test, loss_test, test_labels = compute_attack_components(model_k_ret, test_loader)\n",
    "                logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_ret, forget_loader)\n",
    "                attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                      forget_labels, test_labels[rand_idxs])\n",
    "                auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "                adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "                mia_aucs_k_ret.append(100.0*auc)\n",
    "                mia_advs_k_ret.append(100.0*adv)\n",
    "\n",
    "\n",
    "            # Anonymizing D and training M_k on D_k\n",
    "            mean_anonymize_time = np.mean(runtimes_k)\n",
    "            std_anonymize_time = np.std(runtimes_k)\n",
    "            mean_train_k_time = np.mean(runtimes_train_k)\n",
    "            std_train_k_time = np.std(runtimes_train_k)\n",
    "            mean_train_k_acc = np.mean(train_accs_k)\n",
    "            std_train_k_acc = np.std(train_accs_k)\n",
    "            mean_test_k_acc = np.mean(test_accs_k)\n",
    "            std_test_k_acc = np.std(test_accs_k)\n",
    "            mean_mia_k_auc = np.mean(mia_aucs_k)\n",
    "            std_mia_k_auc = np.std(mia_aucs_k)\n",
    "            mean_mia_k_adv = np.mean(mia_advs_k)\n",
    "            std_mia_k_adv = np.std(mia_advs_k)\n",
    "\n",
    "            # Finetuning M_k on D\n",
    "            mean_finetune_D_time = np.mean(runtimes_train_k_D)\n",
    "            std_finetune_D_time = np.std(runtimes_train_k_D)\n",
    "            mean_finetune_D_train_acc = np.mean(train_accs_k_D)\n",
    "            std_finetune_D_train_acc = np.std(train_accs_k_D)\n",
    "            mean_finetune_D_test_acc = np.mean(test_accs_k_D)\n",
    "            std_finetune_D_test_acc = np.std(test_accs_k_D)\n",
    "            mean_finetune_D_mia_auc = np.mean(mia_aucs_k_D)\n",
    "            std_finetune_D_mia_auc = np.std(mia_aucs_k_D)\n",
    "            mean_finetune_D_mia_adv = np.mean(mia_advs_k_D)\n",
    "            std_finetune_D_mia_adv = np.std(mia_advs_k_D)\n",
    "\n",
    "            # Finetuning M_k on D_ret\n",
    "            mean_finetune_D_ret_time = np.mean(runtimes_train_k_ret)\n",
    "            std_finetune_D_ret_time = np.std(runtimes_train_k_ret)\n",
    "            mean_finetune_D_ret_train_acc = np.mean(retain_accs_k_ret)\n",
    "            std_finetune_D_ret_train_acc = np.std(retain_accs_k_ret)\n",
    "            mean_finetune_D_ret_forget_acc = np.mean(forget_accs_k_ret)\n",
    "            std_finetune_D_ret_forget_acc = np.std(forget_accs_k_ret)\n",
    "            mean_finetune_D_ret_test_acc = np.mean(test_accs_k_ret)\n",
    "            std_finetune_D_ret_test_acc = np.std(test_accs_k_ret)\n",
    "            mean_finetune_D_ret_mia_auc = np.mean(mia_aucs_k_ret)\n",
    "            std_finetune_D_ret_mia_auc = np.std(mia_aucs_k_ret)\n",
    "            mean_finetune_D_ret_mia_adv = np.mean(mia_advs_k_ret)\n",
    "            std_finetune_D_ret_mia_adv = np.std(mia_advs_k_ret)\n",
    "\n",
    "\n",
    "            # Print the results\n",
    "            print('----------------------------------------')\n",
    "            print('k=', k, 'Fine-tuning epochs=', ft_epochs)\n",
    "            print('----------------------------------------')\n",
    "            print('-----Anonymizing D and training M_k on D_k-----')\n",
    "            print('Anonymizing D time:{:0.2f}(±{:0.2f})'.format(mean_anonymize_time, std_anonymize_time))\n",
    "            print('Training M_k on D_k time:{:0.2f}(±{:0.2f})'.format(mean_train_k_time, std_train_k_time))\n",
    "            print('Train AUC:{:0.2f}(±{:0.2f})%'.format(mean_train_k_acc, std_train_k_acc))\n",
    "            print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_test_k_acc, std_test_k_acc))\n",
    "            print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_auc, std_mia_k_auc))\n",
    "            print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_adv, std_mia_k_adv))\n",
    "\n",
    "            print('-----Finetuning M_k on D-----')\n",
    "            print('Training M_k on D time:{:0.2f}(±{:0.2f})'.format(mean_finetune_D_time, std_finetune_D_time))\n",
    "            print('Train AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_train_acc, std_finetune_D_train_acc))\n",
    "            print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_test_acc, std_finetune_D_test_acc))\n",
    "            print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_auc, std_finetune_D_mia_auc))\n",
    "            print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_adv, std_finetune_D_mia_adv))\n",
    "\n",
    "            print('-----Finetuning M_k on D_ret-----')\n",
    "            print('Finetuning M_k on D_retain time:{:0.2f}(±{:0.2f}) seconds'.format(mean_finetune_D_ret_time, std_finetune_D_ret_time))\n",
    "            print('Retain AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc))\n",
    "            print('Forget AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc))\n",
    "            print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc))\n",
    "            print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc))\n",
    "            print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv))\n",
    "            print('----------------------------------------')\n",
    "\n",
    "            # Save to CSV\n",
    "            csv_anonymize_file_path = 'results/credit/mlp_mk={}_dk_fr={}.csv'.format(k, forget_ratio)\n",
    "            csv_finetune_D_file_path = 'results/credit/mlp_mk={}_d_fr={}_epochs={}.csv'.format(k, forget_ratio, ft_epochs)\n",
    "            csv_finetune_D_ret_file_path = 'results/credit/mlp_mk={}_dret_fr={}_epochs={}.csv'.format(k, forget_ratio, ft_epochs)\n",
    "\n",
    "            # Writing to CSV for anonymizing, finetuning on D, and finetuning on D_ret\n",
    "            with open(csv_anonymize_file_path, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                writer.writerow(['Anonymizing Time', mean_anonymize_time, std_anonymize_time])\n",
    "                writer.writerow(['Training M_k on D_k Time', mean_train_k_time, std_train_k_time])\n",
    "                writer.writerow(['Train AUC', mean_train_k_acc, std_train_k_acc])\n",
    "                writer.writerow(['Test AUC', mean_test_k_acc, std_test_k_acc])\n",
    "                writer.writerow(['MIA AUC', mean_mia_k_auc, std_mia_k_auc])\n",
    "                writer.writerow(['MIA Advantage', mean_mia_k_adv, std_mia_k_adv])\n",
    "\n",
    "            with open(csv_finetune_D_file_path, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                writer.writerow(['Training M_k on D Time', mean_finetune_D_time, std_finetune_D_time])\n",
    "                writer.writerow(['Train AUC', mean_finetune_D_train_acc, std_finetune_D_train_acc])\n",
    "                writer.writerow(['Test AUC', mean_finetune_D_test_acc, std_finetune_D_test_acc])\n",
    "                writer.writerow(['MIA AUC', mean_finetune_D_mia_auc, std_finetune_D_mia_auc])\n",
    "                writer.writerow(['MIA Advantage', mean_finetune_D_mia_adv, std_finetune_D_mia_adv])\n",
    "\n",
    "            with open(csv_finetune_D_ret_file_path, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "                writer.writerow(['Finetuning M_k on D_retain Time', mean_finetune_D_ret_time, std_finetune_D_ret_time])\n",
    "                writer.writerow(['Retain AUC', mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc])\n",
    "                writer.writerow(['Forget AUC', mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc])\n",
    "                writer.writerow(['Test AUC', mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc])\n",
    "                writer.writerow(['MIA AUC', mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc])\n",
    "                writer.writerow(['MIA Advantage', mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0a885",
   "metadata": {},
   "source": [
    "# Differential privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4912ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_ratio = 0.5\n",
    "# Step 1: k-anonymize and prepare D_k\n",
    "ft_epochs_list = [5]\n",
    "m = int(len(y_train)*forget_ratio)\n",
    "idxs = np.arange(len(y_train))\n",
    "random.shuffle(idxs)\n",
    "\n",
    "retain_idxs = idxs[m:]\n",
    "forget_idxs = idxs[:m]\n",
    "X_retain = X_train[retain_idxs]\n",
    "y_retain = y_train[retain_idxs]\n",
    "X_forget = X_train[forget_idxs]\n",
    "y_forget = y_train[forget_idxs]\n",
    "retain_dataset = TensorDataset(torch.tensor(X_retain, dtype=torch.float32), torch.tensor(y_retain, dtype=torch.int64))\n",
    "forget_dataset = TensorDataset(torch.tensor(X_forget, dtype=torch.float32), torch.tensor(y_forget, dtype=torch.int64))\n",
    "\n",
    "# Create DataLoader instances\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True)\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for ft_epochs in ft_epochs_list:\n",
    "    EPS = [0.5]\n",
    "    for eps in EPS:\n",
    "        dp_train_data = pd.read_csv('dp_data/GiveMeSomeCredit/dp_credit_eps={}.csv'.format(eps), sep=',')\n",
    "        # Drop useless columns\n",
    "        dp_train_data.dropna(inplace=True)\n",
    "        # convert the income column to 0 or 1 and then drop the column for the feature vectors\n",
    "        # creating the feature vector \n",
    "        X_train_dp = dp_train_data.drop('SeriousDlqin2yrs', axis =1)\n",
    "        # target values\n",
    "        y_train_dp = dp_train_data['SeriousDlqin2yrs'].values\n",
    "        # pass the data through the full_pipeline\n",
    "        X_train_dp = SC.fit_transform(X_train_dp)\n",
    "        # Create TensorDatasets\n",
    "        train_dataset_k = TensorDataset(torch.tensor(X_train_dp, dtype=torch.float32), torch.tensor(y_train_dp, dtype=torch.int64))\n",
    "        train_loader_k = DataLoader(train_dataset_k, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        train_accs_k = []\n",
    "        test_accs_k = []\n",
    "        mia_aucs_k = []\n",
    "        mia_advs_k = []\n",
    "        runtimes_train_k = []\n",
    "\n",
    "        train_accs_k_D = []\n",
    "        test_accs_k_D = []\n",
    "        mia_aucs_k_D = []\n",
    "        mia_advs_k_D = []\n",
    "        runtimes_train_k_D = []\n",
    "\n",
    "        retain_accs_k_ret = []\n",
    "        forget_accs_k_ret = []\n",
    "        test_accs_k_ret = []\n",
    "        mia_aucs_k_ret = []\n",
    "        mia_advs_k_ret = []\n",
    "        runtimes_train_k_ret = []\n",
    "\n",
    "        for r in range(n_repeat):\n",
    "            torch.cuda.empty_cache()\n",
    "            model_k = copy.deepcopy(initial_model)\n",
    "            optimizer = optim.Adam(model_k.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k = train_model(model_k, train_loader_k, test_loader, criterion, optimizer, \n",
    "                            max_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                            patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt_train = t1- t0\n",
    "            runtimes_train_k.append(rt_train)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k.eval()\n",
    "            #Accuracy\n",
    "            train_acc = auc_score(model_k, train_loader)\n",
    "            test_acc = auc_score(model_k, test_loader)\n",
    "            train_accs_k.append(100.0*train_acc)\n",
    "            test_accs_k.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            idxs = np.arange(len(test_dataset))\n",
    "            random.shuffle(idxs)\n",
    "            rand_idxs = idxs[:m]\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k.append(100.0*auc)\n",
    "            mia_advs_k.append(100.0*adv)\n",
    "\n",
    "            model_k_D = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_D.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_D = train_model(model_k_D, train_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_D.append(rt)\n",
    "\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_D.eval()\n",
    "            #Accuracy\n",
    "            train_acc = auc_score(model_k_D, train_loader)\n",
    "            test_acc = auc_score(model_k_D, test_loader)\n",
    "            train_accs_k_D.append(100.0*train_acc)\n",
    "            test_accs_k_D.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_D, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_D, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_D.append(100.0*auc)\n",
    "            mia_advs_k_D.append(100.0*adv)\n",
    "\n",
    "            model_k_ret = copy.deepcopy(model_k)\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer = optim.Adam(model_k_ret.parameters(), lr=lr)\n",
    "            t0 = time.time()\n",
    "            model_k_ret = train_model(model_k_ret, retain_loader, test_loader, criterion, optimizer, \n",
    "                                ft_epochs, device=device, verbose_epoch = int(max_epochs/10), \n",
    "                                  patience = patience)\n",
    "\n",
    "            t1 = time.time()\n",
    "            rt = t1-t0\n",
    "            runtimes_train_k_ret.append(rt)\n",
    "            # Evaluate the model accuracy, and MIA\n",
    "            model_k_ret.eval()\n",
    "            #Accuracy\n",
    "            retain_acc = auc_score(model_k_ret, retain_loader)\n",
    "            forget_acc = auc_score(model_k_ret, forget_loader)\n",
    "            test_acc = auc_score(model_k_ret, test_loader)\n",
    "            retain_accs_k_ret.append(100.0*retain_acc)\n",
    "            forget_accs_k_ret.append(100.0*forget_acc)\n",
    "            test_accs_k_ret.append(100.0*test_acc)\n",
    "            #MIA\n",
    "            logits_test, loss_test, test_labels = compute_attack_components(model_k_ret, test_loader)\n",
    "            logits_forget, loss_forget, forget_labels = compute_attack_components(model_k_ret, forget_loader)\n",
    "            attack_result = tf_attack(logits_forget, logits_test[rand_idxs], loss_forget, loss_test[rand_idxs], \n",
    "                                  forget_labels, test_labels[rand_idxs])\n",
    "            auc = attack_result.get_result_with_max_auc().get_auc()\n",
    "            adv = attack_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n",
    "            mia_aucs_k_ret.append(100.0*auc)\n",
    "            mia_advs_k_ret.append(100.0*adv)\n",
    "\n",
    "\n",
    "        # Anonymizing D and training M_k on D_k\n",
    "        mean_train_k_time = np.mean(runtimes_train_k)\n",
    "        std_train_k_time = np.std(runtimes_train_k)\n",
    "        mean_train_k_acc = np.mean(train_accs_k)\n",
    "        std_train_k_acc = np.std(train_accs_k)\n",
    "        mean_test_k_acc = np.mean(test_accs_k)\n",
    "        std_test_k_acc = np.std(test_accs_k)\n",
    "        mean_mia_k_auc = np.mean(mia_aucs_k)\n",
    "        std_mia_k_auc = np.std(mia_aucs_k)\n",
    "        mean_mia_k_adv = np.mean(mia_advs_k)\n",
    "        std_mia_k_adv = np.std(mia_advs_k)\n",
    "\n",
    "        # Finetuning M_k on D\n",
    "        mean_finetune_D_time = np.mean(runtimes_train_k_D)\n",
    "        std_finetune_D_time = np.std(runtimes_train_k_D)\n",
    "        mean_finetune_D_train_acc = np.mean(train_accs_k_D)\n",
    "        std_finetune_D_train_acc = np.std(train_accs_k_D)\n",
    "        mean_finetune_D_test_acc = np.mean(test_accs_k_D)\n",
    "        std_finetune_D_test_acc = np.std(test_accs_k_D)\n",
    "        mean_finetune_D_mia_auc = np.mean(mia_aucs_k_D)\n",
    "        std_finetune_D_mia_auc = np.std(mia_aucs_k_D)\n",
    "        mean_finetune_D_mia_adv = np.mean(mia_advs_k_D)\n",
    "        std_finetune_D_mia_adv = np.std(mia_advs_k_D)\n",
    "\n",
    "        # Finetuning M_k on D_ret\n",
    "        mean_finetune_D_ret_time = np.mean(runtimes_train_k_ret)\n",
    "        std_finetune_D_ret_time = np.std(runtimes_train_k_ret)\n",
    "        mean_finetune_D_ret_train_acc = np.mean(retain_accs_k_ret)\n",
    "        std_finetune_D_ret_train_acc = np.std(retain_accs_k_ret)\n",
    "        mean_finetune_D_ret_forget_acc = np.mean(forget_accs_k_ret)\n",
    "        std_finetune_D_ret_forget_acc = np.std(forget_accs_k_ret)\n",
    "        mean_finetune_D_ret_test_acc = np.mean(test_accs_k_ret)\n",
    "        std_finetune_D_ret_test_acc = np.std(test_accs_k_ret)\n",
    "        mean_finetune_D_ret_mia_auc = np.mean(mia_aucs_k_ret)\n",
    "        std_finetune_D_ret_mia_auc = np.std(mia_aucs_k_ret)\n",
    "        mean_finetune_D_ret_mia_adv = np.mean(mia_advs_k_ret)\n",
    "        std_finetune_D_ret_mia_adv = np.std(mia_advs_k_ret)\n",
    "\n",
    "        # Print the results\n",
    "        print('----------------------------------------')\n",
    "        print('Epsilon=', eps, 'Fine-tuning epochs=', ft_epochs)\n",
    "        print('----------------------------------------')\n",
    "        print('-----Anonymizing D and training M_dp on D_dp-----')\n",
    "        print('Training M_k on D_k time:{:0.2f}(±{:0.2f})'.format(mean_train_k_time, std_train_k_time))\n",
    "        print('Train AUC:{:0.2f}(±{:0.2f})%'.format(mean_train_k_acc, std_train_k_acc))\n",
    "        print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_test_k_acc, std_test_k_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_auc, std_mia_k_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_mia_k_adv, std_mia_k_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D-----')\n",
    "        print('Training M_k on D time:{:0.2f}(±{:0.2f})'.format(mean_finetune_D_time, std_finetune_D_time))\n",
    "        print('Train AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_train_acc, std_finetune_D_train_acc))\n",
    "        print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_test_acc, std_finetune_D_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_auc, std_finetune_D_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_mia_adv, std_finetune_D_mia_adv))\n",
    "\n",
    "        print('-----Finetuning M_k on D_ret-----')\n",
    "        print('Finetuning M_k on D_retain time:{:0.2f}(±{:0.2f}) seconds'.format(mean_finetune_D_ret_time, std_finetune_D_ret_time))\n",
    "        print('Retain AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc))\n",
    "        print('Forget AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc))\n",
    "        print('Test AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc))\n",
    "        print('MIA AUC:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc))\n",
    "        print('MIA Advantage:{:0.2f}(±{:0.2f})%'.format(mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv))\n",
    "        print('----------------------------------------')\n",
    "\n",
    "        # Save to CSV\n",
    "        csv_anonymize_file_path = 'results/credit/mlp_mdp_eps={}_fr={}.csv'.format(eps, forget_ratio)\n",
    "        csv_finetune_D_file_path = 'results/credit/mlp_mdpd_eps={}_fr={}_epochs={}.csv'.format(eps, forget_ratio, ft_epochs)\n",
    "        csv_finetune_D_ret_file_path = 'results/credit/mlp_mdpret_eps={}_fr={}_epochs={}.csv'.format(eps, forget_ratio, ft_epochs)\n",
    "\n",
    "        # Writing to CSV for anonymizing, finetuning on D, and finetuning on D_ret\n",
    "        with open(csv_anonymize_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Training M_k on D_k Time', mean_train_k_time, std_train_k_time])\n",
    "            writer.writerow(['Train AUC', mean_train_k_acc, std_train_k_acc])\n",
    "            writer.writerow(['Test AUC', mean_test_k_acc, std_test_k_acc])\n",
    "            writer.writerow(['MIA AUC', mean_mia_k_auc, std_mia_k_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_mia_k_adv, std_mia_k_adv])\n",
    "\n",
    "        with open(csv_finetune_D_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Training M_k on D Time', mean_finetune_D_time, std_finetune_D_time])\n",
    "            writer.writerow(['Train AUC', mean_finetune_D_train_acc, std_finetune_D_train_acc])\n",
    "            writer.writerow(['Test AUC', mean_finetune_D_test_acc, std_finetune_D_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_mia_auc, std_finetune_D_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_mia_adv, std_finetune_D_mia_adv])\n",
    "\n",
    "        with open(csv_finetune_D_ret_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Metric', 'Mean', 'Standard Deviation'])\n",
    "            writer.writerow(['Finetuning M_k on D_retain Time', mean_finetune_D_ret_time, std_finetune_D_ret_time])\n",
    "            writer.writerow(['Retain AUC', mean_finetune_D_ret_train_acc, std_finetune_D_ret_train_acc])\n",
    "            writer.writerow(['Forget AUC', mean_finetune_D_ret_forget_acc, std_finetune_D_ret_forget_acc])\n",
    "            writer.writerow(['Test AUC', mean_finetune_D_ret_test_acc, std_finetune_D_ret_test_acc])\n",
    "            writer.writerow(['MIA AUC', mean_finetune_D_ret_mia_auc, std_finetune_D_ret_mia_auc])\n",
    "            writer.writerow(['MIA Advantage', mean_finetune_D_ret_mia_adv, std_finetune_D_ret_mia_adv])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
